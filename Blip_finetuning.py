# -*- coding: utf-8 -*-
"""blip_finetuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eVyPVQtDehpufwcO0bXaEaRwL-giTRS_
"""

# ***************추가할것들*****************
# 그래프 추가 or 결과값 텍스트로 저장(lr, batch, epoch, 결과값들).
# 가중치 저장 에폭마다  --> 에폭 크게 돌리고 좋은거 하나 고르기
# 하이퍼파라미터 변경해가면서 실험해보기! --> 뭐가 제일 좋을라나?

# **************완료*********************
# train val test 비율 나누기
# bleu score추가
# 테스트셋 만들기...?  ---> case별로 20장씩 총 100장

# 필요한 라이브러리 및 패키지 설치
!pip install git+https://github.com/huggingface/transformers.git@main
!pip install -q datasets

from torchvision.datasets import CocoCaptions
from torch.utils.data import Dataset, DataLoader

# COCO 데이터셋 다운로드 및 로드
coco_dataset = CocoCaptions(root="/content/drive/MyDrive/abnormal_dataset", annFile="/content/drive/MyDrive/train_abnormal_dataset.json", transform=None)
test_dataset = CocoCaptions(root="/content/drive/MyDrive/abnormal_dataset", annFile="/content/drive/MyDrive/test_abnormal_dataset.json", transform=None)

# Hugging Face의 transformers 라이브러리에서 AutoProcessor 및 BlipForConditionalGeneration 불러오기
from transformers import AutoProcessor, BlipForConditionalGeneration

# Processor 및 모델 초기화
train_processor = AutoProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
valid_processor = AutoProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
test_processor = AutoProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

# 이미지 캡션 데이터셋 클래스 정의
class ImageCaptioningDataset(Dataset):
    def __init__(self, dataset, processor):
        self.dataset = dataset
        self.processor = processor

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        image, captions = self.dataset[idx]
        text = captions[0]

        encoding = self.processor(images=image, text=text, padding="max_length", return_tensors="pt")
        encoding = {k: v.squeeze() for k, v in encoding.items()}
        return encoding

from torch.utils.data import random_split

# Calculate the sizes of train, valid, and test sets
total_size = len(coco_dataset)
train_size = int(0.9 * total_size)
valid_size = (total_size - train_size)

# Use random_split to create train, valid, and test datasets
train_dataset, valid_dataset = random_split(coco_dataset, [train_size, valid_size])

# Initialize datasets and loaders for train, valid, and test sets
train_dataset = ImageCaptioningDataset(train_dataset, train_processor)
valid_dataset = ImageCaptioningDataset(valid_dataset, valid_processor)
test_dataset = ImageCaptioningDataset(test_dataset, test_processor)

train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=4)
valid_dataloader = DataLoader(valid_dataset, shuffle=False, batch_size=4)  # No need to shuffle the validation set
test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=1)  # No need to shuffle the test set

train_size = len(train_dataset)
valid_size = len(valid_dataset)
test_size = len(test_dataset)

print(total_size)
print(f"Number of samples in the training dataset: {train_size}")
print(f"Number of samples in the validation dataset: {valid_size}")
print(f"Number of samples in the test dataset: {test_size}")

import gc
import torch

gc.collect()
torch.cuda.empty_cache()

# AdamW 옵티마이저 및 장치 설정
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# Training loop
print_frequency = 100

import nltk
from nltk.translate.bleu_score import sentence_bleu
from transformers import AutoTokenizer  # Assuming you are using a pretrained model from Hugging Face

nltk.download("punkt")
tokenizer = AutoTokenizer.from_pretrained("Salesforce/blip-image-captioning-base")

# Lists to store metrics for plotting
train_losses = []
train_perplexities = []
val_losses = []
val_perplexities = []
val_bleu_scores = []

for epoch in range(20):
    print("Training Epoch:", epoch)
    total_loss = 0.0
    total_perplexity = 0.0
    num_batches = len(train_dataloader)

    model.train()
    for idx, batch in enumerate(train_dataloader):
        torch.cuda.empty_cache()
        input_ids = batch.pop("input_ids").to(device)
        pixel_values = batch.pop("pixel_values").to(device)

        outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=input_ids)

        loss = outputs.loss

        perplexity = torch.exp(loss)
        total_perplexity += perplexity.item()

        total_loss += outputs.loss.item()

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        # if (idx + 1) % print_frequency == 0 or idx == num_batches - 1:
        #     print(f"Training Batch {idx}/{num_batches}, Loss: {outputs.loss.item()}, Perplexity: {perplexity}")

    average_loss = total_loss / num_batches
    average_perplexity = total_perplexity / num_batches
    print(f"Training Epoch {epoch}, Average Loss: {average_loss}, Average Perplexity: {average_perplexity}")

    # Validation loop
    model.eval()
    with torch.no_grad():
        total_val_loss = 0.0
        total_val_perplexity = 0.0
        total_bleu_score = 0.0
        num_val_batches = len(valid_dataloader)

        for val_idx, val_batch in enumerate(valid_dataloader):
            val_input_ids = val_batch.pop("input_ids").to(device)
            val_pixel_values = val_batch.pop("pixel_values").to(device)

            val_outputs = model(input_ids=val_input_ids, pixel_values=val_pixel_values, labels=val_input_ids)

            val_loss = val_outputs.loss

            val_perplexity = torch.exp(val_loss)
            total_val_perplexity += val_perplexity.item()

            total_val_loss += val_outputs.loss.item()

            # Decode model output and reference to sentences
            decoded_sentences = [tokenizer.decode(output, skip_special_tokens=True) for output in val_outputs.logits.argmax(dim=-1)]
            references = [tokenizer.decode(ref, skip_special_tokens=True) for ref in val_input_ids]

            # Calculate BLEU score
            bleu_score = 0.0
            for i in range(len(decoded_sentences)):
                bleu_score += sentence_bleu([references[i].split()], decoded_sentences[i].split())
            bleu_score /= len(decoded_sentences)
            total_bleu_score += bleu_score

        average_val_loss = total_val_loss / num_val_batches
        average_val_perplexity = total_val_perplexity / num_val_batches
        average_bleu_score = total_bleu_score / num_val_batches
        print(f"Validation Epoch {epoch}, Average Loss: {average_val_loss}, Average Perplexity: {average_val_perplexity}, Average BLEU Score: {average_bleu_score}")

    # Append metrics for plotting
    train_losses.append(average_loss)
    train_perplexities.append(average_perplexity)
    val_losses.append(average_val_loss)
    val_perplexities.append(average_val_perplexity)
    val_bleu_scores.append(average_bleu_score)

    # Save the model at the end of each epoch
    model_save_path = f"/content/drive/MyDrive/blip_finetuning_weight2/{epoch}_blip_model_weights.pth"
    model.save_pretrained(model_save_path)
    print(f"Model weights saved to {model_save_path}")

    # Save the model if it performs better on validation
    # if epoch == 0 or average_val_loss < best_val_loss:
    #     best_val_loss = average_val_loss
    #     best_model_save_path = "/content/drive/MyDrive/blip_finetuning_weight/4best_blip_model_weights.pth"
    #     model.save_pretrained(best_model_save_path)

print("Training complete.")

import matplotlib.pyplot as plt

# Plotting
epochs = range(1, 21)  # Assuming 5 epochs
plt.figure(figsize=(10, 6))

# Plot training and validation loss
plt.subplot(2, 1, 1)
plt.plot(epochs, train_losses, label='Training Loss', marker='o')
plt.plot(epochs, val_losses, label='Validation Loss', marker='o')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plot training and validation perplexity
plt.subplot(2, 1, 2)
plt.plot(epochs, train_perplexities, label='Training Perplexity', marker='o')
plt.plot(epochs, val_perplexities, label='Validation Perplexity', marker='o')
plt.title('Training and Validation Perplexity')
plt.xlabel('Epochs')
plt.ylabel('Perplexity')
plt.legend()

plt.tight_layout()
plt.show()

# Plot validation BLEU score
plt.figure(figsize=(10, 6))
plt.plot(epochs, val_bleu_scores, label='Validation BLEU Score', marker='o')
plt.title('Validation BLEU Score')
plt.xlabel('Epochs')
plt.ylabel('BLEU Score')
plt.legend()
plt.show()

import torch
from torch.utils.data import DataLoader
from transformers import AutoProcessor, BlipForConditionalGeneration
import nltk
from nltk.translate.bleu_score import sentence_bleu
from transformers import AutoTokenizer  # Assuming you are using a pretrained model from Hugging Face
nltk.download("punkt")

tokenizer = AutoTokenizer.from_pretrained("Salesforce/blip-image-captioning-base")
# Load the saved model weights
model_path = "/content/drive/MyDrive/blip_finetuning_weight/5_blip_model_weights.pth"
model = BlipForConditionalGeneration.from_pretrained(model_path)
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# Test loop
model.eval()
with torch.no_grad():
    total_test_loss = 0.0
    total_test_perplexity = 0.0
    total_bleu_score_test = 0.0
    num_test_batches = len(test_dataloader)

    # Open a file to save predictions and ground truth captions
    with open("/content/drive/MyDrive/blip_finetuning_weight/test_predictions.txt", "w") as predictions_file:
        for test_idx, test_batch in enumerate(test_dataloader):
            test_input_ids = test_batch.pop("input_ids").to(device)
            test_pixel_values = test_batch.pop("pixel_values").to(device)

            test_outputs = model(input_ids=test_input_ids, pixel_values=test_pixel_values, labels=test_input_ids)

            test_loss = test_outputs.loss

            test_perplexity = torch.exp(test_loss)
            total_test_perplexity += test_perplexity.item()

            total_test_loss += test_outputs.loss.item()

            # Decode model output and reference to sentences
            decoded_sentences_test = [tokenizer.decode(output, skip_special_tokens=True) for output in test_outputs.logits.argmax(dim=-1)]
            references_test = [tokenizer.decode(ref, skip_special_tokens=True) for ref in test_input_ids]

            # Calculate BLEU score for the test set
            # bleu_score_test = 0.0
            # for i in range(len(decoded_sentences_test)):
            #     bleu_score_test += sentence_bleu([references_test[i].split()], decoded_sentences_test[i].split())
            # bleu_score_test /= len(decoded_sentences_test)
            # total_bleu_score_test += bleu_score_test

            # Calculate BLEU score for the test set
            bleu_score_test = 0.0
            for i in range(len(decoded_sentences_test)):
                bleu_score_test += sentence_bleu([references_test[i].split()], decoded_sentences_test[i].split(), weights=(0.25, 0.25, 0.25, 0.25))
            bleu_score_test /= len(decoded_sentences_test)
            total_bleu_score_test += bleu_score_test

            # Save predictions and ground truth captions to the file
            for ref, pred in zip(references_test, decoded_sentences_test):
                predictions_file.write(f"Reference: {ref}\nPrediction: {pred}\n\n")

    average_test_loss = total_test_loss / num_test_batches
    average_test_perplexity = total_test_perplexity / num_test_batches
    average_bleu_score_test = total_bleu_score_test / num_test_batches
    print(f"Test Results: Average Loss: {average_test_loss}, Average Perplexity: {average_test_perplexity}, Average BLEU Score: {average_bleu_score_test}")

print("Test complete.")

import torch
from torch.utils.data import DataLoader
from transformers import AutoProcessor, BlipForConditionalGeneration
import nltk
from nltk.translate.bleu_score import sentence_bleu
from transformers import AutoTokenizer  # Assuming you are using a pretrained model from Hugging Face

nltk.download("punkt")

tokenizer = AutoTokenizer.from_pretrained("Salesforce/blip-image-captioning-base")

# Load the saved model weights
model_path = "/content/drive/MyDrive/blip_finetuning_weight/5_blip_model_weights.pth"
model = BlipForConditionalGeneration.from_pretrained(model_path)
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# Test loop
model.eval()
with torch.no_grad():
    total_test_loss = 0.0
    total_test_perplexity = 0.0
    total_bleu_score_test_1gram = 0.0
    total_bleu_score_test_2gram = 0.0
    total_bleu_score_test_3gram = 0.0
    total_bleu_score_test_4gram = 0.0
    num_test_batches = len(test_dataloader)

    # Open a file to save predictions and ground truth captions
    with open("/content/drive/MyDrive/blip_finetuning_weight/test_predictions.txt", "w") as predictions_file:
        for test_idx, test_batch in enumerate(test_dataloader):
            test_input_ids = test_batch.pop("input_ids").to(device)
            test_pixel_values = test_batch.pop("pixel_values").to(device)

            test_outputs = model(input_ids=test_input_ids, pixel_values=test_pixel_values, labels=test_input_ids)

            test_loss = test_outputs.loss

            test_perplexity = torch.exp(test_loss)
            total_test_perplexity += test_perplexity.item()

            total_test_loss += test_outputs.loss.item()

            # Decode model output and reference to sentences
            decoded_sentences_test = [tokenizer.decode(output, skip_special_tokens=True) for output in test_outputs.logits.argmax(dim=-1)]
            references_test = [tokenizer.decode(ref, skip_special_tokens=True) for ref in test_input_ids]

            # Calculate BLEU scores for the test set
            bleu_score_test_1gram = 0.0
            bleu_score_test_2gram = 0.0
            bleu_score_test_3gram = 0.0
            bleu_score_test_4gram = 0.0

            for i in range(len(decoded_sentences_test)):
                # Calculate BLEU scores for different n-grams
                bleu_score_test_1gram += sentence_bleu([references_test[i].split()], decoded_sentences_test[i].split(), weights=(1, 0, 0, 0))
                bleu_score_test_2gram += sentence_bleu([references_test[i].split()], decoded_sentences_test[i].split(), weights=(0.5, 0.5, 0, 0))
                bleu_score_test_3gram += sentence_bleu([references_test[i].split()], decoded_sentences_test[i].split(), weights=(0.33, 0.33, 0.33, 0))
                bleu_score_test_4gram += sentence_bleu([references_test[i].split()], decoded_sentences_test[i].split(), weights=(0.25, 0.25, 0.25, 0.25))

            bleu_score_test_1gram /= len(decoded_sentences_test)
            bleu_score_test_2gram /= len(decoded_sentences_test)
            bleu_score_test_3gram /= len(decoded_sentences_test)
            bleu_score_test_4gram /= len(decoded_sentences_test)

            total_bleu_score_test_1gram += bleu_score_test_1gram
            total_bleu_score_test_2gram += bleu_score_test_2gram
            total_bleu_score_test_3gram += bleu_score_test_3gram
            total_bleu_score_test_4gram += bleu_score_test_4gram

            # Save predictions and ground truth captions to the file
            for ref, pred in zip(references_test, decoded_sentences_test):
                predictions_file.write(f"Reference: {ref}\nPrediction: {pred}\n\n")

    average_test_loss = total_test_loss / num_test_batches
    average_test_perplexity = total_test_perplexity / num_test_batches
    average_bleu_score_test_1gram = total_bleu_score_test_1gram / num_test_batches
    average_bleu_score_test_2gram = total_bleu_score_test_2gram / num_test_batches
    average_bleu_score_test_3gram = total_bleu_score_test_3gram / num_test_batches
    average_bleu_score_test_4gram = total_bleu_score_test_4gram / num_test_batches

    print(f"Test Results: Average Loss: {average_test_loss}, Average Perplexity: {average_test_perplexity}")
    print(f"Average BLEU Score (1-gram): {average_bleu_score_test_1gram}")
    print(f"Average BLEU Score (2-gram): {average_bleu_score_test_2gram}")
    print(f"Average BLEU Score (3-gram): {average_bleu_score_test_3gram}")
    print(f"Average BLEU Score (4-gram): {average_bleu_score_test_4gram}")

print("Test complete.")

import torch
from torch.utils.data import DataLoader
from transformers import AutoProcessor, BlipForConditionalGeneration
import nltk
from nltk.translate.bleu_score import sentence_bleu
from transformers import AutoTokenizer  # Assuming you are using a pretrained model from Hugging Face
nltk.download("punkt")

tokenizer = AutoTokenizer.from_pretrained("Salesforce/blip-image-captioning-base")
# Load the saved model weights
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# Test loop
model.eval()
with torch.no_grad():
    total_test_loss = 0.0
    total_test_perplexity = 0.0
    total_bleu_score_test = 0.0
    num_test_batches = len(test_dataloader)

    # Open a file to save predictions and ground truth captions
    with open("/content/drive/MyDrive/blip_finetuning_weight/basemodel_test_predictions.txt", "w") as predictions_file:
        for test_idx, test_batch in enumerate(test_dataloader):
            test_input_ids = test_batch.pop("input_ids").to(device)
            test_pixel_values = test_batch.pop("pixel_values").to(device)

            test_outputs = model(input_ids=test_input_ids, pixel_values=test_pixel_values, labels=test_input_ids)

            test_loss = test_outputs.loss

            test_perplexity = torch.exp(test_loss)
            total_test_perplexity += test_perplexity.item()

            total_test_loss += test_outputs.loss.item()

            # Decode model output and reference to sentences
            decoded_sentences_test = [tokenizer.decode(output, skip_special_tokens=True) for output in test_outputs.logits.argmax(dim=-1)]
            references_test = [tokenizer.decode(ref, skip_special_tokens=True) for ref in test_input_ids]

            # Calculate BLEU score for the test set
            bleu_score_test = 0.0
            for i in range(len(decoded_sentences_test)):
                bleu_score_test += sentence_bleu([references_test[i].split()], decoded_sentences_test[i].split())
            bleu_score_test /= len(decoded_sentences_test)
            total_bleu_score_test += bleu_score_test

            # Save predictions and ground truth captions to the file
            for ref, pred in zip(references_test, decoded_sentences_test):
                predictions_file.write(f"Reference: {ref}\nPrediction: {pred}\n\n")

    average_test_loss = total_test_loss / num_test_batches
    average_test_perplexity = total_test_perplexity / num_test_batches
    average_bleu_score_test = total_bleu_score_test / num_test_batches
    print(f"Test Results: Average Loss: {average_test_loss}, Average Perplexity: {average_test_perplexity}, Average BLEU Score: {average_bleu_score_test}")

print("Test complete.")

import torch
from PIL import Image
import matplotlib.pyplot as plt
from transformers import BlipForConditionalGeneration, AutoProcessor

# 모델 및 이미지 경로 정의
model_save_path = "/content/drive/MyDrive/blip_finetuning_weight/5_blip_model_weights.pth"
image_path = "/content/images.jpeg"

# 저장된 모델 불러오기
loaded_model = BlipForConditionalGeneration.from_pretrained(model_save_path)
loaded_model.eval()

# Processor 불러오기
processor = AutoProcessor.from_pretrained("Salesforce/blip-image-captioning-base")

# 이미지 로드 및 전처리
image = Image.open(image_path)

# 캡션 생성
inputs = processor(images=image, padding="max_length", return_tensors="pt")
with torch.no_grad():
    outputs = loaded_model.generate(**inputs)

# 생성된 캡션 디코딩
generated_captions = processor.batch_decode(outputs, skip_special_tokens=True)

# 생성된 캡션 출력
print("Generated Captions:")
for i, caption in enumerate(generated_captions):
    print(f"{i + 1}. {caption}")

plt.imshow(image)
plt.axis('off')
plt.show()

1. a black car and a white car collided on the road.

1. a woman wearing a black top is lying on the stairs.


a man wearing black pants is lying down in the hallway.